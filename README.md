**Realtime Visualization of Streaming Sensor Data**
==============
By <a href= "https://github.com/cam9">Cam Lunt</a> and <a href= "https://github.com/reedery">Ryan Reede</a>

**Goals**

To illustrate what goes on behind the scenes of Virtual and Augmented Reality (specifically mobile VR such at Cardboard of Galaxy Gear VR) applications. There are two javascript visiualizations present:
- The **first** represents an arrow vector in 3D worldspace that originates at the origin (0,0,0) and terminates at a variable location determined by how much translational movement is invoked on the device. Pushing the tablet forward quickly will produce a long arrow moving into negative z space. Likewise, pulling the tablet towards the user with a significant amount of force will draw the arrow into positive z space. As the tablet deaccelerates, the arrow sizes down back towards the origin. In Augmented Reality applications, to render an image on top of the real world at a specific, fixed location, compensating for the lateral motion visualized here in realtime is key. 
- The **second** visualization renders a sphere designed to represent a 3D envorironment in an application. A Hemisphere Light will render a portion of the sphere orange based upon the orientation of the tablet. If the tablet is facing up, the top of the sphere will be lit orange while the rest is blue. As the tablet is rotated and placed face-down, the orange highlighted portion of the sphere moves towards the bottom of the sphere. This visualization is intended to represent how VR and AR applications determine what subset of pixels in a 3D environment are to be rendered by the GPU and fed to the display. Rendering the entire environment is a significant waste of computing power as only a fraction of the 3D world will ever be in the field of view of the user at a given instant. The falloff of the orange portion of the sphere as it blends in with the blue represents how graphics processing algorithms prioritze certain pixels. Pixels at the edge of or just outisde of the field of view are rendered at a lower quality to prepare for the possibility that the user moves their head into that position. VR and AR applications must be able to maintain around 75Hz of refresh speed to prevent motion sickness and mimic reality, so prioritizing and managing computing resources becames extremely important.  


**Process**

- Translational (acceleromter) and Rotational (gyroscope) data generated by Project Tango device
- Data sent via socket to Kafka Producer
- Data queued and sent to Kafka Consumer (Jetty Server)
- Logic preformed in Jetty backend and sent via html5 WebSockets to browser
- THREE.js visualization of data 


Resources:
  - <a href="http://www.javacirecep.com/internet/java-a-simple-websocket-example/">Javaci Recap Simple Websocket Example<a>
  - <a href="https://github.com/jetty-project/embedded-jetty-websocket-examples">Embedded Jetty WebSocket Examples<a>
 
 
 
